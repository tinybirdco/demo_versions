# Tinybird Versions Demo

This Demo started life as a small Tinybird data project called 'Epimetheus' to show how you could parse a stream of metrics into a responsive grafana dashboard at high volumes.

Epimetheus, meaning 'Hindsight', was husband to Pandora and brother to Prometheus.

It now also has reference examples of integrating Tinybird with Version control as an educational resource. 

We hope you find it useful ^^

---

## Introduction

This repository demonstrates how you might work with Tinybird ['Versions'](https://www.tinybird.co/versions) as a Data Engineer. Versions is a feature designed to help you iterate your production data projects safely and easily using standard software development practices like read-only production deployments, integrated version control, pull requests, and continuous integration & deployment.

The repo is intended to be instructional as a combination data project and some example pull requests for Versioning changes already completed in the Repo history. The Data Project and Versioning implementation will be discussed separately, followed by instructions for how you can use this project as a starting point to make your own changes and learn along the way.

### Introducing the Data Use Case

The data use case is modeled on a typical metric reporting situation: a single aggregated stream of X different metric events from Y source machines which are differentiated by Z tagged attributes. The Metrics are randomly named, with variable value distribution, and apart from a few common tags like 'environment' and 'server' the tags are also randomly generated to simulate a broader loading pattern.

A sample of the data generated is [here](https://github.com/tinybirdco/demo_versions/blob/main/datasources/fixtures/events.ndjson); this sample is used for fixture testing and looks like this:

``` json
{"id": "KEEPALIVE", "ts": "2023-09-18T11:38:29.645Z", "val": "1", "att": {"env": "dev", "server": "sparrow", "datacenter": "chopin"}}
```
The `id` and `val` fields are for the actual Metric; the `ts` is millisecond accurate, and the `att` is a key:value map of attributes tagged to the Metric.

The output of the data use case is to have a Grafana dashboard which provides data aggregations over selectable 1 second, 5 minute, and 15 minute intervals, and also selectable filters based on converting selected tags to columns in the table.

The Grafana dashboard json file definition is provided in the repository [here](https://github.com/tinybirdco/demo_versions/blob/main/grafana_dashboard.json) for use with a free Grafana Cloud account, and instructions are provided for connecting it to your Tinybird Workspace.

The data is generated by a simple Python script included with the project [here](https://github.com/tinybirdco/demo_versions/blob/main/datagen.py), using minimal dependencies so the user can focus on the outcome of changes to Versions of the Data Project. 
While we send a few thousand events per second directly to Tinybird in this demo, rest assured it would work just as well with Kafka, Big Query, Snowflake, S3 or other data sources we support.

### Introducing the Versions Use Case

The Versions use case starts with this simple 'production' data project and then walks through how you can iterate on it safely by keeping production protected and incrementally preparing, testing and applying your changes through a flexible but integrated development process.

It simulates the scenario where a Data Engineer needs to pick up an existing data project, add a new metric, and make it available as an API for downstream use, but also include it in the filters available on the project monitoring dashboard in Grafana.

Therefore this demo will guide you through a process of making two common changes to the Data Project:

1. Adding a simple Pipe for a new API to the Workspace
2. Add a column to a Materialized View, the downstream Datasource, and the Pipe that publishes an API from it

These have already been executed once in the data project and show in the history of commits and pull requests, we'll review those changes and then guide you through making your own.

### What we will cover

This Guide is now broken into several stages:

1. [Initialize the Data Project](#initialise-the-data-project): Preparing the initial fork of this Repository, the Tinybird Workspace, the Grafana Dashboard, and producing some data to test it is all working as a baseline starting point.
2. [Productionisation](#productionisation): Setting the Tinybird Workspace to be production 'read-only', cloning a child Workspace 'Environment' with a copy of some Prod data to develop your changes, and how to them remove the child Workspace when you are done with it.
3. [Reviewing Examples](#reviewing-examples): We will then briefly review pull requests that have already been completed in this Repository before taking through making your own in the following steps.
4. [A simple change end-to-end](#a-simple-change-adding-a-new-pipe-and-api-endpoint): Create a new Pipe using this proecss, bring these changes through the typical software development lifecycle process in Github and promote them to the production Workspace.
5. [A complex change end-to-end](#a-complex-change-adding-a-field-to-a-datasource-from-a-materialized-view): Then we will use this process to do a more complex change where we need to Version specific endpoints so we can evolve the schema of a Datasource and the Materialized View which produces it.

## Setup

### Requirements

You will need an empty [Tinybird Workspace](https://ui.tinybird.co/), a [Grafana Cloud deployment](https://grafana.com/auth/sign-up/create-user), [this repo](https://github.com/tinybirdco/demo_versions), and the [Tinybird CLI](https://pypi.org/project/tinybird-cli/) installed in a Python virtual environment. Note that introductory versions of Tinybird and Grafana are both free. The instructions below will guide you through each of these steps as you go.

This project makes use of Github Pull Requests to show the full development lifecycle, so if your Github account has particular restrictions please consider any impact that may have.

I have prepared this Demo using Visual Studio Code on a MacBook Pro M1 using Python 3.11.2 - other tools and versions may work but results may vary.

## Initialise the Data Project

### Get the Repo

First things first, you will want to **fork** this repository into your own Github account, and then clone it down to your local machine. You could also **clone** it to your machine and push it to another repository if you wish, but you will not be able to make pull requests back to this parent repository to test the Github actions, so please use one of these options.

Either way, you will want a copy of it in your IDE of choice, such as Visual Studio Code on your local machine, and the linked copy in your Github account.

### Prepare Python

Next, create a Python virtual environment for your project and install the Python requirements:

```shell
pip install -r requirements.txt
```

These requirements include the Tinybird CLI along with standard tools like requests and faker.

### Create a Tinybird Workspace

Go sign up for [Tinybird](https://ui.tinybird.co/), or create a new Workspace in your existing account.

### Authenticate to Tinybird via CLI

We will then authenticate to your Tinybird Workspace in the CLI we installed in the Python requirements:

* Go to your Tinybird Workspace in the UI
* Open up the `Tokens` option in the left sidebar menu
* Select the `admin <user email>` token, and copy it to your clipboard
* In your terminal, change directory to the root of the project where the datagen.py script is located.
* Execute the following command, and paste in the user admin token:

```shell
tb auth
```

Note: This will create a `.tinyb` file which contains information to connect to your Workspace, including the security Token. This file is already included in the `.gitignore` in this project so it should not be picked up by any commits you make, but if you modified `.gitignore` or aren't using it please ensure you are careful with your security Tokens around version control.

### Provide the deployment secret to Github

This provides Github with the Token necessary to make changes to your Tinybird Workspace so it can test your changes and promote them when you merge the pull requests. 

Set `TB_ADMIN_TOKEN` in Github > Your Repo > Settings > Secrets and Variables > Actions
Use the same Admin token you applied to `tb auth`.

Note that this Token has admin rights to the Tinybird Workspace, and is accessible from Github actions, so be sure to review any pull requests that your repository may receive to ensure the Token is used with care in the CICD process.

### Connect Tinybird to your Github repo

Now we need to instruct Tinybird and Github to be friends, which will trigger the initial push of the Data project to Tinybird ready to use.

In the project root where you executed `tb auth` now run

```shell
tb init --git
```

In a typical development scenario, you might have started with an empty Tinybird workspace and developed the Data Project in the UI before deciding it was mature enough to commit to Git and mark as ready for production software development practices. This is exactly what I have done in this case, you are just skipping that part of the process and deploying my project which has already been iterated on some of the APIs as a working example.

Depending on how you set up the repo, you may have to push the initial commit to Github yourself, but if you forked the repo and cloned it down then it should already be there.

### Generate some data

Now is a good time to get some initial data into your data project.
The datagen.py script has no unusual dependencies, and the default volumes are a good starting point. It will use the Tinybird Token you have already setup in the project in the earlier step to push data to the Workspace.

You can run it in a Terminal with

```shell
python datagen.py
```

The swiches are optional, but they are:

* `--metrics` The number of random metrics to generate, defaults to 10
* `--batch` The number of events to push in each batch, defaults to 3_000. These are generated in bulk with millisecond timestamps and small gaps between each batch to provide depth to the timeseries
* `--events` The total number of events to generate before stopping the script, defaults to 100_000. Set it to a very high number for practically infinite generation.

At this point you could jump into the Tinybird UI and test the API endpoints to see that data coming through.
Of particular relevance to the Grafana dashboard are:

* event_volume - provides a per-minute rollup of the total number of events being processed
* events_by_tags_chart - provides the main datasource for the grafana chart with filter parameters
* column_listings - provides the listing for each filter for Grafana to populate the filters dropdowns
* latest_att_keys - provides the datagen script with the listing of the current random Metrics in use

### Set up Grafana Cloud

Now that you have the data project setup, and some data in the API endpoints, you can deploy the Grafana Dashboard which pulls from them to have the whole baseline data project up and running.

Note: If you are just interested in Versions and not particularly in Grafana, then you don't need to do this step, but it's a good way to validate that the changes you have made are working with a nice visualisation.

* Sign up for [Grafana Cloud](https://grafana.com/auth/sign-up/create-user), if you don't already have an account
* In the Administration Settings for the account, under Default Preferences, you will want it to use a default Timezone of 'Coordinated Universal Time'. Otherwise it'll default to your local Timezone and the graphs will be offset.

Next we need to create our Datasource connection to Tinybird

* Create a new Datasource using the 'JSON API' plugin, name it `Tinybird`, and initially point the URL at `https://api.tinybird.co/v0/pipes/events_by_tags_chart.json` so we can test the connection.
* Add a Custom HTTP Header to the Datasource; the Header is `Authorization`, and the Value is `Bearer <Token>` where `<Token>` has at least Read access to all your API endpoints in the Data Project. You can use the `Admin <user email>` Token if you wish, though we would not recommend this in a true production setting.
* It should connect correctly and give you a success message if the Endpoint is correctly published and your Token has read access to it, otherwise use the error messages to troubleshoot.
* Once the connection is working correctly, change the URL to be the root of the API service at `https://api.tinybird.co` - this allows us to specify the API Endpoints as path attributes appended to this base URL within the Dashboard.
* Finally, in your browser URL for this configuration webpage you will see that it contains the UID for this new Datasource you have created - something like `https://epimetheus.grafana.net/connections/datasources/edit/fe99b71a-6316-4ea7-a6c1-74390e5ad63d`. Copy the UUID from after `edit/` as we will need to use it to configure our Dashboard.

Next we need to update our Dashboard definition to point to this Datasource using the new UID.
Unfortunately you can't do this easily via the Grafana Cloud UI, so we do it in the json file.

* Open the `grafana_dashboard.json` file in your favourite IDE or text editor
* On line 28 you will see reference to a UID for the Datasource for the Grafana Dashboard, it's something like "fe99b71a-6316-4ea7-a6c1-74390e5ad63d"
* There are several references to this UID in the file, find and replace all of them with the UID you took from your new Datasource.
* Save the file and exit.

Now we can deploy our Dashboard

* In Grafana Cloud, navigate to Home > Dashboards > All Dashboards
* Create a New Dashboard using the button in the top right corner, use the option to Import a Dashboard
* Upload the Dashboard definition from the root of the repository.
* Check you can see the data that you've been generating. You may wish to change the time frame to be short, like 30 minutes UTC, to see the recent data.
* If you see an error on one of the panels, click on it to see the URI it is using, and check that it matches the URI being published as an API endpoint by Tinybird - if you have made version changes, you may have to update the Dashboard to talk to the version you are currently publishing.

### Data Project initialisation is now complete

Great, now you have replicated the data project at the same point as our Version 2 release. Now you can move on to Productionising it by making the 'main' Environment read-only, and looking at how to use child Environments and your IDE / Git workflow to iterate with your own Version changes.

## Productionisation

Now that we have our base Data Project in place, we can examine the developer workflow that it was built with.

To be clear, this is not the _only_ way to use Tinybird with Versions - in fact, many customers use multiple individual Workspaces for larger projects across different dev, staging, and prod deployments.
However in this case, because our Data Project is quite simple, using the integrated 'Environments' feature strikes a good balance between functionality and accessibility.

### Marking the main Workspace as 'read-only'

In Settings in the UI, under advanced, hit the tick box to mark this Workspace as read-only. Simple.

This will put you into the Environments UI, where you see a new dropdown to select which Environment you want to be in - 'main' is the default environment which is protected as read-only.

### Using the Playground feature

Before we go and make a child Environment, if you find that you simply want to run a couple of queries without making any persistent changes, you can use the Playground feature in the main sidebar. This creates a transient Pipe which you can work within and share with others, but you cannot change Datasources or APIs from it.

It is useful for troubleshooting or quick investigations, but if you want a true Sandbox to develop more complex changes you probably want to use Environments as outlined in this guide.

### Creating a child Environment

In the drop down, click on create new Environment.
Name it something easy to type, as you'll likely end up typing it in the CLI now and then.

Choose to copy the last partition so you have some sample data to develop against - don't worry if you don't do this - you can run a query against a Datasource within a child Environment to load more data from the parent Environment whenever you like.

This process is exactly the same as what the CI integration does when it creates a temporary Environment to test your pull requests, so you can use this to test your changes locally before committing them to Git.

### Removing a child Environment

When you are finished with an Environment, you can remove it easily.

Go into the Settings cog while in the Environment, under the Advanced tab there is an option to delete it.
You can also do this in the CLI or API if you want to.

## Reviewing Examples

Now let's have a look at examples of the some changes already executed in the repository to get a feel for what will be happening when you make your own changes later in the guide.

### Review adding a Pipe

In [pull request #2](https://github.com/tinybirdco/demo_versions/pull/2) we added a Pipe with a new API to the data project, the purpose of this API is to return the listing of items for each dropdown filter in the Grafana dashboard.

The PR is super simple - there is a single commit with a single [file](https://github.com/tinybirdco/demo_versions/pull/1/files) added which describes the Pipe and a new Token with read permissions.

The PR was automatically tested with the templated Github actions, of particular interest are the automatic pipe regression tests in [here](https://github.com/tinybirdco/ci/blob/main/.github/workflows/ci.yml). In this case, because the Pipe doesn't interact with any of the existing APIs or Pipes, there's nothing it could really break, and thus it's a 'simple' additive change.

We also do not make use of the advanced Versioning features which automatically create version-numbered instances of Pipes and Datasources, and scripts to migrate between versions (we'll do this in the complex example).

### Concepts: Data Project Version and Resource Version

This section summarises documention about Custom deploymen strategies [here](https://www.tinybird.co/docs/guides/deployment-strategies.html?highlight=cd%20deploy%20sh#custom-deployments) in the Tinybird Documentation

The next example introduces versioning at the overall data project level, and at the individual resource (Pipe, etc.) level within the data project.

The data project version follows [semantic versioning](https://semver.org/) and is recorded in the `.tinyenv` file for the current Git branch you are working on. When you want to increment your overall project version, usually because you need to migrate resources like a Datasource between incompatible states (like changing a column in the schema), you update this file to a new version and create a `ci-deploy.sh` and/or `cd-deploy.sh` script in a matching subdirectory of `deploy'. This script is used to execute the commands to migrate your resources from the previous version to this new one, by running things like SQL queries or populate commands.

So, if your `.tinyenv` file has `VERSION 1.0.0` in it, then your script to migrate to that version during deployment should be in `deploy/1.0.0/cd-deploy.sh`. You can also have a `ci-deploy.sh` file if you need commands for the CI portion of your workflow, but in this case we've only needed them for CD.

The resource version is an integer specifically set in the file for a given resource, like your Pipes or Datasources.
This explicitly instructs Tinybird to create a new copy of that resource with your changes during the deployment, and appends the version to the name, like `my_pipe__v1` if you put `VERSION 1` in the pipe definition.
This is intended to allow you fine grained control over which 'Version' of the resource your downstream service users receive. You could prepare a new version and carefully test it before migrating to it, for example, or you could prepare a new Datasource version in one PR, and then make another PR to integrate it with a pulished API endpoint.

You can also then retire previous versions when you no longer need them, ideally using CLI commands during some future version release process in the cd-deploy.sh script.

### Review adding a column to a Materialized View and Datasource

This is a more complex, but very common change, where you are already extracting information from a 'raw' Datasource using a Materialized View, and you want to do something more with it.

In this case, consider the initial or Version 0 events Datasource in the data project - we extract and index the 'environment' and 'server' tags, but not the 'datacenter' tag. Imagine you as a developer want this to be available in the Grafana monitoring dashboard, or that you want to filter by datacenter in the API as a part of your project responsibilities.

In [this](https://github.com/tinybirdco/demo_versions/pull/3) pull request we again have one commit with all our changes, however you may break this over several commits and merge requests if that suited your development flow.

Now let us look at the changes in the PR to see in more detail what Versioning adds to the situation.

* in the [Datasource definition](https://github.com/tinybirdco/demo_versions/pull/3/files#diff-ca5bc6b1a49a2881f57cc19c41025c8fedaae9ca565ccf972bce1940efd7cd34) we start by adding (or incrementing) the `VERSION` tag to 1, indicating that the system should create a `__v1` copy of the Datasource with these new changes. This allows us to migrate our downstream consumers of the API to this newer version once we are happy with it. We then also make the necessary changes to the Pipe to handle `Datacenter` being indexed as a separate column by adding it to the schema and the Sorting Key.
* Following these same principles, we also update the other Pipes to both increment the version and support the new 'Datacenter' tag being indexed.
* Because we are responsibile developers, we also ensure that the [data fixture](https://github.com/tinybirdco/demo_versions/pull/3/files#diff-9d6cb03cb8a0067c51452329d3cabb5f3cc68e73dac4f110dd7e5aca4fc48260) for the raw datasource is updated to reflect this new column being included for any tests we may want to run.
* Next let us look at the [deployment script](https://github.com/tinybirdco/demo_versions/pull/3/files#diff-8912b3a44500141326bf6f4a17204aa7ec38c10db5b6bef68ee11288e95f6cfd) - the actual commands in cd-deploy.sh in this case are very simple - they instruct Tinybird to deploy this version, and then run a 'populate' command on that Pipe to ensure the new Datasource version is backfilled with all necessary data.
* Finally, we have also incremented the .tinyenv [file](https://github.com/tinybirdco/demo_versions/pull/3/files#diff-83cf7b769928eb93c3d9aa54cae5a7b4ba84375ebcfd461680aaf15d7a4efb36) to `VERSION=0.0.1`

Now when this pull request was merged, Tinybird has follow the instructions to create the `__v1` Pipes and Datasources, and execute the `cd-deploy.sh` migration script to populate the Materialized Views.

We would then take steps to migrate downstream applications like our Grafana dashboard to use these endpoints, and we could then retire them when no longer needed - generally via further commands in some future pull request.

## Making your own changes

Now let us walk through you making your own changes to your own Workspace.

### A simple change: Adding a new Pipe and API Endpoint

So in this case we will make the simplest change possible to get familiar with the process.
This is to add a new Pipe to the workspace with publishes an API endpoint - it has no dependencies other than an existing Datasource, and requires no changes to any other parts of the Data Project and therefore does not yet require you to use the explicit Versioning controls.

#### Creating and validating our changes

Firstly, create a new Environment from 'main', name it something easy to type, and choose to import the last data partition. I will use the Environment name 'new_pipe' here.

You will quickly be dropped into an editable copy of Prod which will visually track any changes you make.

Create a new Pipe, and you'll see that it is marked with a blue dot in the UI to show it has changed from the 'main' Environment.
This query creates a five minute rollup count grouped by the Environment tag as a simple example.

``` SQL
SELECT 
    toStartOfFiveMinutes(Timestamp) as Timestamp,
    Environment,
    count() as cnt
FROM events_by_tags
group by Environment, Timestamp
```

Name the Pipe `events_by_env_5m` to reflect its purpose
Name the Node in the Pipe `events_by_env_5m_0` - a useful convention to append the node number '0' to the parent name for ease of identification.

Publish this Query as an API and check the output is as you expect it to be.
Select Node '0' that you just renamed to be published. You can execute the test query in your browser if you like - as this child Environment is in all respects a clone of the 'main' environment, you could even push fresh data to it for validation using the Tokens if you wanted to.

#### Pulling the changes into our IDE

Now that we have the new API endpoint that we want to promote to production running in our child Environment, its time to pull it into our IDE to commit it to version control.

First, in your IDE ensure you are up to date with your 'main' Git branch, and create a new Git branch for this feature. You can name it the same as your Environment if you like for ease of reference, so I will use 'new_pipe' here.

Then open up the terminal - if you are running the datagen script you can stop it for now.

Use the Tinybird CLI to switch to the child Environment you created:

`tb env use new_pipe`

Note: When you switch to the child Environment the CLI switches out the local Tokens to point to it, which is what the datagen script is using to write events to Tinybird. Therefore if it is running you will stop seeing the events writing to your 'main' Environment and therefore the Grafana dashboard!

Then, pull all the changes from the child Environment.

`tb pull --auto -force`

The 'auto' switch instructs the CLI to distribute the files into the directory structure, the 'force' flag instructs it to overwrite any existing files with updated versions pulled from your changes in the UI.

You will potentially see a number of files change, depending on what you have touched in the UI.
We are particularly interested in our new Pipe file which should appear in `pipes/events_by_env_5m.pipe`. You can safely discard the changes to the other files for now.

You could have, of course, directly written this file in the IDE as it is quite a simple definition, but the purpose of this guide is to learn how to use this development flow for more complex cases where the UI helps with data exploration and ensuring that your more complex changes are correct to save you from many commits to fix small typos and other things that might happen in local development.

#### Creating and exploring a pull request

Now that we have the change we want to promote in our local feature branch, we should commit and push it to our repository. You can use your favourite interface here, I quite like the VSCode Github integration.

Once you have committed this new file to the branch, and pushed the branch to the repository, VSCode will usually prompt you to create a pull request for it. You can use this process, or create it through other means like the Git CLI or in the Github browser UI.

The important thing here is that you are using your regular development processes to create the pull request.

Once the PR is created, Github will automatically start the CI validation tests that come baked into the templates we provide. You can track these in the Pull request, and investigate the various checks it is doing.
If you have copied the Pipe exactly, it should pass without any concerns as there can be no regressions on a new API Pipe.

In the background it is actually creating a temporary CI Environment, loading in your changes, and running a bunch of automated tests, then removing the Environment and reporting in detail on success and failure.

#### Merging the changes to production

You can monitor the progress of checking the pull request either in the Github UI or your preferred method. Once it comes back green it indicates you are ready to merge this change and push it to the production Workspace.

Merge the PR and watch the production deployment flow.
When it is successful you can return to the Tinybird UI, switch back to the 'main' Workspace and observe that your new API endpoint Pipe is available for use.

#### Cleaning up our development Environment

Now that we have iterated 'main' to a new Version, it is a good idea to clean up our child Environment. This way you don't leave resources lying around, but also it is now stale as production has moved to a new Version.

Before deleting the child Environment, you will want to switch your CLI back to the 'main' with `tb env use main`. This is because you are currently using a Token associated with the `new_pipe` child environment, which will no longer be valid when it is deleted.

You can delete the Environment by switching to it in the UI and going to Settings > Advanced, or you can use the CLI.

### A complex change: Adding a field to a Datasource from a Materialized View

Now that we've executed the overall process to prepare, test, and merge a new pull request using the suggested developer workflow, let's apply our new experience to a more complex iterative change.

Following the example earlier where we showed adding 'Datacenter' as in indexed column, this time we will add 'Server'. We've prepared an example PR for you to copy the changes from [here](https://github.com/tinybirdco/demo_versions/pull/3/files), where we added the 'Datacenter' column instead.

In this process, we're going to walk you through the typical UI process for iterating several related Materialized Views, and then pulling the changes down to your IDE. If you already know exactly what changes you want to make, you could just edit the files directly in your IDE, but you wouldn't have the benefit of the UI helping you avoid mistakes before the CICD process checks your code - using the UI is an easy way to avoid typos!

#### UI Walkthrough of a complex change

We're going to give you a summary of steps to follow here, as the exact code examples are in the provided example PR - refer to them or ask us in Slack if you are unsure!

1. In the Tinybird UI, go back to the read-only 'main' Environment
2. Create a new child environment, let's name it 'new_feature'. Ensure you copy the last partition, as before.
3. In the events_by_tags_1s_mv, events_by_tags_5m_mv, and events_by_tags_15m_mv Pipes, unlink the Materialized Views to allow you to edit the Pipes.
4. Because adding a column will change the schema of the downstream Datasources, delete the previously attached Datasources 'events_by_tags_*'. Don't worry, we're about to recreate them as part of our UI changes.
5. Make the changes to the Pipes using the example from PR #3 above. Note the Select, Group By, and ENGINE_SORTING_KEY are updated. Don't worry about the VERSION statement just yet, focus on being the developer in the UI figuring out their SQL changes.
6. Ensure that you are selecting `server` with a lowercase S from the attributes, and creating a column as `Server` with uppercase S to match the other indexed columns.
6. Now recreate the Materialized View using the green publish button. Make sure you reuse the same name, and observe that it'll pick up the new column you've created.
7. Now let's go to the 'events_by_tags_chart' Pipe which has our API for Grafana.
8. In here, add the 'Server' column to the SELECT statement, as well as the filter parameters (copy the environment one and replace with server with a default of 'dove'), and the GROUP BY. Anywhere you see 'Datacenter' you should ensure 'Server' is next to it in each of the Nodes.
9. Your API will instantly update with the new capability, you can check the output by hitting the green 'View API' button and using one of the sample calls to see that the Server is now included in the output.

Now that our UI has the changes we want, and we've tested they work, we can pull those changes down into our IDE

1. In your IDE, make sure you are up to date with your main Git branch, and create a new feature branch to work in.
2. In the Terminal, run the command `tb env use new_feature` to switch your Tinybird CLI to point at your changes
3. In the Terminal again, use the command `tb pull --auto --force` to pull the changes down from the development Environment

Now you should have all the changes from your child Environment in your IDE.
Next we are going to introduce the Versioning controls so that these changes can be rolled out without breaking the existing production applications.

1. In the Pipes and Datasources we have edited, increment the resource Version at the top of the file - it should now be `VERSION 2`
2. Open the `.tinyenv` file and increment the data project Version - `VERSION=0.0.2`
3. Create a new deploy subdirectory matching this version, and copy the cd-deploy.sh file from the previous version into it.
4. Change the `tb pipe populate` commands to match the version that will be created, so they should be like `tb pipe populate events_by_tags_1s_mv__v2 --node events_by_tags_1s_0 --wait`
5. You can compare your changes to those in the files in the example PR [here](https://github.com/tinybirdco/demo_versions/pull/3/files)

Now that we have our versioning instructions in place, you can commit the changes to your branch, push the branch to Github, and create a pull request out of it.
The CI process will then pick that up and attempt to automatically test for regressions.

One regression you are likely to hit is that the endpoint is now responding with more latency because it is fetching more data. This is expected, and you can apply a label to the PR you have created to instruct the tests to ignore this result, the tag is `--assert-time-increase-percentage -1`

Examine the output of the checks to ensure everything looks green, if it is, you can go head and merge the PR and then clean up your development Environment as before.

Now if you generate more data on your `main` Environment and examine your Grafana dashboard, you should see that the `Server` filter is now available and working as expected.
